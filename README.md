
# **2025 Hugging Face Hackathon â€“ ê²½ìƒë¶ë„ì§€ì‚¬ìƒ**

##  **VLA ê¸°ë°˜ ìŒì„±-ì§€ì‹œ 4ìƒ‰ ì–‘ì´ˆ ìë™ ë¶„ë¥˜ ë¬¼ë¥˜ ë¡œë´‡ (Leaderâ€“Follower Imitation Learning Robot)**

#### Developer: ì†ê·œì› (KNU Electronic Engineering)
#### Date : 2025.06.14 ~ 06.15 
---

# 1. Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ”
**ì‚¬ìš©ìì˜ ìŒì„± ì§€ì‹œ â†’ ìƒ‰ìƒ ì¸ì‹ â†’ ë¡œë´‡íŒ” pick â†’ ìƒì ìë™ ë¶„ë¥˜**
ê¹Œì§€ ì´ë¤„ì§€ëŠ” **VLA(Vision-Language-Action) ê¸°ë°˜ ë¬¼ë¥˜ ë¡œë´‡ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

ëŒ€íšŒì˜ í•µì‹¬ ê³¼ì œì˜€ë˜
**ë¦¬ë“œ(Teacher) ë¡œë´‡ì˜ Demonstrationì„ íŒ”ë¡œìš°(Follower) ë¡œë´‡ì´ ëª¨ë°©í•™ìŠµì„ í†µí•´ ì¬í˜„**
í•˜ëŠ” êµ¬ì¡°ë¥¼ ì‹¤ì œ ë¡œë´‡íŒ” + ìŒì„± ì§€ì‹œ í™˜ê²½ì— ì™„ì „íˆ ì ìš©í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
ì´ **3ê°œì˜ ì¹´ë©”ë¼(RGB 2ëŒ€ + Depth 1ëŒ€)** ë¥¼ ì‚¬ìš©í•˜ì—¬  
>â€“ ë‹¤ì–‘í•œ ê°ë„ì˜ ìƒ‰ìƒ ì¸ì‹ ì•ˆì •í™”  
> â€“ ê¹Šì´ ì •ë³´ ê¸°ë°˜ Z-axis pick ë™ì‘ ì•ˆì •ì„±  

ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

> â€œë§í•˜ë©´ ë¡œë´‡ì´ ì›í•˜ëŠ” ìƒ‰ ë¬¼ê±´ì„ ì°¾ì•„ì„œ ìë™ìœ¼ë¡œ ì§‘ì–´ ì˜®ê¸´ë‹¤.â€

ë¼ëŠ” **ì§ê´€ì ì´ê³  ì‚°ì—…ì ì¸ Multimodal Robot Task**ë¥¼ êµ¬í˜„í•´
2025 HuggingFace í•´ì»¤í†¤ì—ì„œ **ê²½ìƒë¶ë„ì§€ì‚¬ìƒ** ì„ ìˆ˜ìƒí–ˆìŠµë‹ˆë‹¤.

<br>

---

#  2. Competition â€“ What is Hugging Face Hackathon 2025?

2025 Hugging Face Hackathonì˜ ë©”ì¸ ì£¼ì œëŠ”:

> **Vision-Language-Actionì„ í™œìš©í•˜ì—¬
> Teacher Robotì˜ ì‹œì—° í–‰ë™ì„ Follower Robotì´ ëª¨ë°©í•˜ë„ë¡ êµ¬í˜„í•˜ë¼.**

ìš”êµ¬ì‚¬í•­:

* Vision + Language + Action ìœµí•©(Multimodal)
* ì‹¤ì œ ë¡œë´‡íŒ” ì œì–´ í¬í•¨
* Teacher â†’ Follower í–‰ë™ í•™ìŠµ êµ¬ì¡°
* Behavior Cloning / VLA Policy ê¸°ë°˜ ëª¨ë¸ êµ¬í˜„
* ì‹¤ì‹œê°„ ë°ëª¨ ê°€ëŠ¥í•´ì•¼ í•¨

ì¦‰, **AI + Robotics + Multimodal** ì„ ëª¨ë‘ ìš”êµ¬í•˜ëŠ”  ëŒ€íšŒì˜€ìŠµë‹ˆë‹¤.

<br>

---

# 3. ì™œ ì´ ì£¼ì œë¥¼ ì„ íƒí–ˆëŠ”ê°€? (Why This Topic?)

### 1) VLA êµ¬ì¡°ë¥¼ ê°€ì¥ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” Task

* Vision â†’ ì–‘ì´ˆ ìƒ‰ ê°ì§€
* Language â†’ ì‚¬ìš©ìì˜ ìŒì„± ì§€ì‹œ
* Action â†’ ë¡œë´‡íŒ” pick & place
  ì´ 3ìš”ì†Œê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ëŠ” êµ¬ì¡°.

### 2) Teacherâ€“Follower íŒ¨í„´ê³¼ ì˜ ë§ëŠ” Motion

Pick ë™ì‘ì˜ trajectoryê°€ ëª…í™•í•˜ì—¬
Followerê°€ imitation learningìœ¼ë¡œ í•™ìŠµí•˜ê³  ì¬í˜„í•˜ê¸°ì— ìµœì .

### 3) ì‚°ì—… í™•ì¥ì„±ì´ ë§¤ìš° ë†’ìŒ

* ë¬¼ë¥˜ í”¼í‚¹ ë¡œë´‡
* ìŒì„± ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬
* ìƒ‰ìƒ/ë¼ë²¨ ê¸°ë°˜ ìë™ ë¶„ë¥˜
  ì´ëŸ° ì‚°ì—… ì‹œë‚˜ë¦¬ì˜¤ì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ì‹œì—°í•  ìˆ˜ ìˆìŒ.

### 4) ì§§ì€ í•´ì»¤í†¤ ê¸°ê°„ ë‚´ ì™„ì„± ê°€ëŠ¥í•œ ìµœì ì˜ MVP

ë°ì´í„° ìˆ˜ì§‘Â·í•™ìŠµÂ·ì œì–´ê¹Œì§€ í•˜ë£¨ë¼ëŠ” ì œí•œ ì‹œê°„ì— ì™„ì„±í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°.

<br>

---

#  4. System Architecture

```
[User Voice]
     â†“
[Speech-to-Text (Whisper)]
     â†“
[Intent Parsing: color("red","yellow","green","blue")]

[RGB Camera #1] â”
                 â”œâ”€â†’ [Vision Encoder (Multi-view RGB + Depth)]
[RGB Camera #2] â”˜

[Depth Camera] â†’ [Depth to Z-Position]

     â†“
[VLA Imitation Policy Model]
     â†“
[IK Solver + Robot Arm Controller]
     â†“
[Pick & Place to Box]

```
---

<br>

# 5. Setup Photo

<p align="center">
  <img src="image/setup.jpg" width="400" height="600">
</p>

---

# ğŸ›  6. Dataset (Teacher Demonstration)

Teacher ë¡œë´‡ì„ ì§ì ‘ ì¡°ì‘í•˜ì—¬ ì•„ë˜ ì •ë³´ë¥¼ ê¸°ë¡:
```
* RGB view 1
* RGB view 2
* Depth frame (Z ì¶• ì •ë°€ ë™ì‘)
* Joint ê°ë„ Î¸(0~5)
* Grip ìƒíƒœ(open/close)
* Episode ë‹¨ìœ„ Action Sequence
* Instruction Text (ì˜ˆ: â€œpick the red candleâ€)
```
```
episode_01_red/
 â”£ rgb_cam1/
 â”£ rgb_cam2/
 â”£ depth/
 â”£ actions.json
 â”£ states.json
 â”— instruction.txt
```

---

# 7. Model â€“ VLA Imitation Policy

### Input

* Vision  
  â€“ RGB Camera #1  
  â€“ RGB Camera #2  
  â€“ Depth Camera(Z-axis for pick height)

* Language  
  â€“ instruction text (â€œpick the blue candleâ€)

* State  
  â€“ joint angles

### Architecture

```
Image Encoder â†’ Image Embedding
Text Encoder â†’ Text Embedding
Concat â†’ Transformer/MLP Policy Head
â†’ Output: next joint angles / Î”pose
```

---

# 8. Training Performance

### 8.1 Training Curves

<p align="center">
  <img src="image/train_result_1.jpg" width="800" height="400">
</p>

âœ” Loss 0.05 ì´í•˜ë¡œ ìˆ˜ë ´
âœ” Grad Norm ì•ˆì •í™”
âœ” Cosine learning rate schedule ì ìš©

---

### 8.2 Joint-Level Action Tracking

<p align="center">
  <img src="image/motor_joint.jpg" width="800">
</p>

Teacher trajectory(íŒŒë€ ì„ )ì™€
Policy Model ì˜ˆì¸¡(ë¹¨ê°„ ì„ )ì´ ê±°ì˜ ì™„ì „íˆ ì¼ì¹˜í•˜ì—¬
ëª¨ë°©ì •í™•ë„ê°€ ë§¤ìš° ë†’ìŒì„ í™•ì¸.

---

#  9. Demonstration Video

> ì‚¬ìš©ì ìŒì„±: â€œíŒŒë€ ì–‘ì´ˆ ë„£ì–´ì¤˜â€
> â†’ STT
> â†’ Vision Target Detection
> â†’ Imitation Policyë¡œ Pick Trajectory ìƒì„±
> â†’ Boxë¡œ ì´ë™ í›„ Place

**Demo Video (MP4)**

<p align="center">
<video src="https://github.com/elecwgs/HuggingFace_Hackathon2025_VLARobot/blob/main/images/result.mp4" controls width="700" height="400"></video>


---

#  10. Result Summary

| í•­ëª©               | ê²°ê³¼                  |
| ---------------- | ------------------- |
| ìŒì„± ì§€ì‹œ ì¸ì‹         | 100%                |
| ì–‘ì´ˆ ìƒ‰ ì¸ì‹          | ì¡°ëª… ë³€í™”ì—ì„œë„ ì•ˆì •ì         |
| Pick & Place ì„±ê³µë¥  | 90% ì´ìƒ              |
| Behavior Cloning | Teacher ë™ì‘ì„ ì •ë°€í•˜ê²Œ ì¬í˜„ |
| í•´ì»¤í†¤              | **ê²½ìƒë¶ë„ì§€ì‚¬ìƒ ìˆ˜ìƒ**      |

---

#  11. Folder Structure

```
huggingface_vla_logistics_robot/
 â”£ images/
 â”£ dataset/
 â”£ model/
 â”£ train_log/
 â”£ README.md
```

---

# âš ï¸ 12. Notice

ë³¸ ì €ì¥ì†ŒëŠ” í•´ì»¤í†¤ ë‹¹ì‹œ ì‚¬ìš©ëœ **ìš´ì˜ ì½”ë“œê°€ ì•„ë‹Œ**,
**êµ¬ì¡°/ì•„ì´ë””ì–´/í•™ìŠµ ê¸°ë¡ì„ ì •ë¦¬í•œ í¬íŠ¸í´ë¦¬ì˜¤ìš© ë¬¸ì„œ ë ˆí¬**ì…ë‹ˆë‹¤.

í™˜ê²½ ì˜ì¡´ì„±ì´ í° ì‹¤ì œ ì½”ë“œ(ì„œë³´ ë³´ì •ê°’, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë“±)ëŠ”
ê³µê°œí•˜ì§€ ì•Šìœ¼ë©°, ì¬í˜„í•˜ë ¤ë©´ ì‚¬ìš©ìê°€ ì§ì ‘ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**English Version:**

This repository does *not* contain the original production code used during the hackathon.
It serves as a **documentation-only portfolio repository**, summarizing the VLA pipeline, dataset design, and system architecture.

---

#  13. Contact

**Developer: ì†ê·œì› (Kyuwon Son)**
ğŸ“§ [dreamsuga1@knu.ac.kr](mailto:dreamsuga1@knu.ac.kr)
ğŸŒ GitHub: [https://github.com/elecwgs](https://github.com/elecwgs)


